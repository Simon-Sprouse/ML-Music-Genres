{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "# Utility: freeze params\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# 1. ResNet-34\n",
    "resnet34 = freeze_model(models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1))\n",
    "print(\"ResNet-34 loaded:\", sum(p.numel() for p in resnet34.parameters())/1e6, \"M params\")\n",
    "\n",
    "# 2. InceptionV3\n",
    "inception = freeze_model(models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1))\n",
    "print(\"InceptionV3 loaded:\", sum(p.numel() for p in inception.parameters())/1e6, \"M params\")\n",
    "\n",
    "# 3. SqueezeNet 1.1\n",
    "squeezenet = freeze_model(models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1))\n",
    "print(\"SqueezeNet loaded:\", sum(p.numel() for p in squeezenet.parameters())/1e6, \"M params\")\n",
    "\n",
    "# 4. EfficientNetV2-S (via timm)\n",
    "efficientnetv2s = freeze_model(timm.create_model(\"tf_efficientnetv2_s_in21k\", pretrained=True))\n",
    "print(\"EfficientNetV2-S loaded:\", sum(p.numel() for p in efficientnetv2s.parameters())/1e6, \"M params\")\n",
    "\n",
    "# 5. MobileNetV3-Small\n",
    "mobilenetv3s = freeze_model(models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1))\n",
    "print(\"MobileNetV3-Small loaded:\", sum(p.numel() for p in mobilenetv3s.parameters())/1e6, \"M params\")\n",
    "\n",
    "print(\"\\nâœ… All models loaded and frozen successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming models are already loaded and frozen:\n",
    "# resnet34, inception, squeezenet, efficientnetv2s, mobilenetv3s\n",
    "\n",
    "# Preprocessing transforms\n",
    "preprocess_224 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "preprocess_299 = transforms.Compose([\n",
    "    transforms.Resize(320),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_embeddings(image_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Run an image through all five frozen models and return embeddings.\n",
    "    Returns a dictionary of tensors keyed by model name.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    embeddings = {}\n",
    "    \n",
    "    # Set all models to eval mode and freeze parameters\n",
    "    resnet34.eval()\n",
    "    inception.eval()\n",
    "    squeezenet.eval()\n",
    "    efficientnetv2s.eval()\n",
    "    mobilenetv3s.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ResNet-34 - Remove final FC layer\n",
    "        x = preprocess_224(img).unsqueeze(0).to(device)\n",
    "        resnet34_feat = torch.flatten(torch.nn.Sequential(*list(resnet34.children())[:-1])(x), 1)\n",
    "        embeddings['resnet34'] = resnet34_feat\n",
    "        \n",
    "        # InceptionV3 - Use forward pass and extract features before final FC\n",
    "        x = preprocess_299(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        ## Replace the fc layer temporarily\n",
    "        original_fc = inception.fc\n",
    "        inception.fc = nn.Identity()\n",
    "        inception_feat = inception(x)\n",
    "        inception.fc = original_fc  # Restore original\n",
    "        embeddings['inceptionv3'] = inception_feat\n",
    "    \n",
    "        \n",
    "        # SqueezeNet 1.1 - Use features + global average pooling\n",
    "        x = preprocess_224(img).unsqueeze(0).to(device)\n",
    "        squeezenet_raw = squeezenet.features(x)  # [1, 512, H, W]\n",
    "        squeezenet_pooled = torch.nn.functional.adaptive_avg_pool2d(squeezenet_raw, (1, 1))  # [1, 512, 1, 1]\n",
    "        squeezenet_feat = torch.flatten(squeezenet_pooled, 1)  # [1, 512]\n",
    "        embeddings['squeezenet'] = squeezenet_feat\n",
    "        \n",
    "        # EfficientNetV2-S - Use forward_features + global average pooling\n",
    "        x = preprocess_224(img).unsqueeze(0).to(device)\n",
    "        effnet_raw = efficientnetv2s.forward_features(x)  # [1, channels, H, W]\n",
    "        effnet_pooled = torch.nn.functional.adaptive_avg_pool2d(effnet_raw, (1, 1))  # [1, channels, 1, 1]\n",
    "        effnet_feat = torch.flatten(effnet_pooled, 1)  # [1, channels]\n",
    "        embeddings['efficientnetv2s'] = effnet_feat\n",
    "        \n",
    "        # MobileNetV3-Small - Use features + global average pooling\n",
    "        x = preprocess_224(img).unsqueeze(0).to(device)\n",
    "        mobilenet_raw = mobilenetv3s.features(x)  # [1, channels, H, W]\n",
    "        mobilenet_pooled = torch.nn.functional.adaptive_avg_pool2d(mobilenet_raw, (1, 1))  # [1, channels, 1, 1]\n",
    "        mobilenet_feat = torch.flatten(mobilenet_pooled, 1)  # [1, channels]\n",
    "        embeddings['mobilenetv3_small'] = mobilenet_feat\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Test the function\n",
    "filename = \"../Results/Dev/rock.00069/aligned_spectrogram_21.png\"\n",
    "embeddings = get_embeddings(filename, device='cpu')\n",
    "\n",
    "print(f\"{'Model Name':<20} | {'Embedding Shape'}\")\n",
    "print(\"-\" * 30)\n",
    "for model_name, embedding in embeddings.items():\n",
    "    print(f\"{model_name:<20} | {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def get_aligned_spectrograms(prefix):\n",
    "    \"\"\"\n",
    "    Given a prefix like '../Results/Dev/rock.00069/aligned_spectrogram',\n",
    "    return all matching files of the form aligned_spectrogram_<n>.png,\n",
    "    sorted numerically by <n>.\n",
    "    \"\"\"\n",
    "    pattern = f\"{prefix}_*.png\"\n",
    "    files = glob.glob(pattern)\n",
    "    files.sort(key=lambda x: int(os.path.splitext(x)[0].split('_')[-1]))\n",
    "    return files\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "prefix = \"../Results/Dev/rock.00069/aligned_spectrogram\"\n",
    "filenames = get_aligned_spectrograms(prefix)\n",
    "\n",
    "print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming models are already loaded and frozen:\n",
    "# resnet34, inception, squeezenet, efficientnetv2s, mobilenetv3s\n",
    "\n",
    "# Preprocessing transforms\n",
    "preprocess_224 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "preprocess_299 = transforms.Compose([\n",
    "    transforms.Resize(320),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def get_embeddings_batch(filenames, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Run a batch of images through all five frozen models and return embeddings.\n",
    "    Returns a dictionary of tensors keyed by model name.\n",
    "    Each tensor has shape [batch_size, feature_dim].\n",
    "    \"\"\"\n",
    "    # Load and preprocess images\n",
    "    imgs_224 = [preprocess_224(Image.open(f).convert(\"RGB\")) for f in filenames]\n",
    "    imgs_299 = [preprocess_299(Image.open(f).convert(\"RGB\")) for f in filenames]\n",
    "\n",
    "    batch_224 = torch.stack(imgs_224).to(device)  # [B, 3, 224, 224]\n",
    "    batch_299 = torch.stack(imgs_299).to(device)  # [B, 3, 299, 299]\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    # Set models to eval\n",
    "    resnet34.eval()\n",
    "    inception.eval()\n",
    "    squeezenet.eval()\n",
    "    efficientnetv2s.eval()\n",
    "    mobilenetv3s.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ResNet-34 - Remove final FC layer\n",
    "        resnet34_feat = torch.flatten(\n",
    "            torch.nn.Sequential(*list(resnet34.children())[:-1])(batch_224), 1\n",
    "        )\n",
    "        embeddings[\"resnet34\"] = resnet34_feat\n",
    "\n",
    "        # InceptionV3 - Replace FC layer with Identity\n",
    "        original_fc = inception.fc\n",
    "        inception.fc = nn.Identity()\n",
    "        inception_feat = inception(batch_299)\n",
    "        inception.fc = original_fc\n",
    "        embeddings[\"inceptionv3\"] = inception_feat\n",
    "\n",
    "        # SqueezeNet - features + GAP\n",
    "        sq_raw = squeezenet.features(batch_224)  # [B, 512, H, W]\n",
    "        sq_pooled = torch.nn.functional.adaptive_avg_pool2d(sq_raw, (1, 1))\n",
    "        embeddings[\"squeezenet\"] = torch.flatten(sq_pooled, 1)\n",
    "\n",
    "        # EfficientNetV2-S - forward_features + GAP\n",
    "        eff_raw = efficientnetv2s.forward_features(batch_224)\n",
    "        eff_pooled = torch.nn.functional.adaptive_avg_pool2d(eff_raw, (1, 1))\n",
    "        embeddings[\"efficientnetv2s\"] = torch.flatten(eff_pooled, 1)\n",
    "\n",
    "        # MobileNetV3-Small - features + GAP\n",
    "        mob_raw = mobilenetv3s.features(batch_224)\n",
    "        mob_pooled = torch.nn.functional.adaptive_avg_pool2d(mob_raw, (1, 1))\n",
    "        embeddings[\"mobilenetv3_small\"] = torch.flatten(mob_pooled, 1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "embeddings = get_embeddings_batch(filenames, device=\"cpu\")\n",
    "\n",
    "print(f\"{'Model Name':<20} | {'Embedding Shape'}\")\n",
    "print(\"-\" * 40)\n",
    "for model_name, embedding in embeddings.items():\n",
    "    print(f\"{model_name:<20} | {embedding.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cnn-embeddings)",
   "language": "python",
   "name": "cnn-embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
