{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fit UMAP To Data + Save Manifold at Low Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import joblib  # for saving the UMAP model\n",
    "\n",
    "# --- CONFIG ---\n",
    "reduced_dir = Path(\"../Results/EmbeddingDataReduced\")\n",
    "umap_dir = Path(\"../Results/EmbeddingDataUMAP\")\n",
    "umap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# UMAP parameters (tunable by user)\n",
    "n_neighbors = 11\n",
    "min_dist = 0.1\n",
    "n_components = 10   # change to 3 if you want 3D embeddings\n",
    "metric = \"euclidean\"\n",
    "random_state = 42\n",
    "\n",
    "# --- COLLECT REDUCED BATCH FILES ---\n",
    "batch_files = sorted(reduced_dir.glob(\"batch_*.pt\"))\n",
    "print(f\"Found {len(batch_files)} reduced batch files\")\n",
    "\n",
    "# --- INIT UMAP ---\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    min_dist=min_dist,\n",
    "    n_components=n_components,\n",
    "    metric=metric,\n",
    "    random_state=random_state,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# --- FIT UMAP ON ALL DATA ---\n",
    "print(\"Fitting UMAP model...\")\n",
    "# Load all batches into memory for fit (UMAP does not support partial_fit)\n",
    "all_data = []\n",
    "for f in batch_files:\n",
    "    batch_tensor = torch.load(f, weights_only=True)\n",
    "    all_data.append(batch_tensor.numpy())\n",
    "all_data = np.vstack(all_data)\n",
    "print(f\"Total data for UMAP fit: {all_data.shape}\")\n",
    "\n",
    "umap_model.fit(all_data)\n",
    "print(\"UMAP model trained.\")\n",
    "\n",
    "# --- SAVE UMAP MODEL ---\n",
    "joblib.dump(umap_model, umap_dir / \"umap_model.pkl\")\n",
    "print(\"Saved UMAP model.\")\n",
    "\n",
    "# --- TRANSFORM & SAVE EACH BATCH ---\n",
    "for f in batch_files:\n",
    "    print(f\"Transforming {f.name} ...\")\n",
    "    batch_tensor = torch.load(f, weights_only=True)\n",
    "    reduced = umap_model.transform(batch_tensor.numpy())\n",
    "    reduced_tensor = torch.from_numpy(reduced).to(torch.float32)\n",
    "\n",
    "    out_file = umap_dir / f.name\n",
    "    torch.save(reduced_tensor, out_file)\n",
    "    print(f\"Saved UMAP batch to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Test UMAP data was saved properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIG ---\n",
    "reduced_dir = Path(\"../Results/EmbeddingDataUMAP\")\n",
    "\n",
    "# --- CHECK ---\n",
    "reduced_files = sorted(reduced_dir.glob(\"batch_*.pt\"))\n",
    "if reduced_files:\n",
    "    first_file = reduced_files[0]\n",
    "    print(f\"Loading {first_file.name} ...\")\n",
    "    reduced_tensor = torch.load(first_file, weights_only=True)\n",
    "\n",
    "    print(f\"Type: {type(reduced_tensor)}\")\n",
    "    print(f\"Shape: {reduced_tensor.shape}\")\n",
    "    print(f\"Dtype: {reduced_tensor.dtype}\")\n",
    "    print(f\"First 3 rows:\\n{reduced_tensor[:3]}\")\n",
    "    print(f\"Min/max values: {reduced_tensor.min().item():.6f} / {reduced_tensor.max().item():.6f}\")\n",
    "else:\n",
    "    print(\"No reduced batch files found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Run HDBSCAN To Detect Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # registers 3D projection\n",
    "import hdbscan\n",
    "from collections import Counter\n",
    "\n",
    "# --- CONFIG ---\n",
    "umap_dir = Path(\"../Results/EmbeddingDataUMAP\")\n",
    "\n",
    "# --- LOAD ALL BATCHES ---\n",
    "batch_files = sorted(umap_dir.glob(\"batch_*.pt\"))\n",
    "all_embeds = []\n",
    "\n",
    "for f in batch_files:\n",
    "    print(f\"Loading {f.name} ...\")\n",
    "    batch_tensor = torch.load(f, weights_only=True)\n",
    "    all_embeds.append(batch_tensor.numpy())\n",
    "\n",
    "all_embeds = np.vstack(all_embeds)  # shape: (total_points, n_dim)\n",
    "print(\"Final shape:\", all_embeds.shape)\n",
    "\n",
    "# --- RUN HDBSCAN ---\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=30,  # smaller clusters allowed\n",
    "    min_samples=10        # fewer points needed to avoid noise\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(all_embeds)\n",
    "counts = Counter(cluster_labels)\n",
    "\n",
    "\n",
    "# --- SUMMARY STATISTICS ---\n",
    "total_points = len(cluster_labels)\n",
    "noise_points = counts.get(-1, 0)\n",
    "\n",
    "# Exclude noise for cluster size stats\n",
    "cluster_sizes = [count for cid, count in counts.items() if cid != -1]\n",
    "num_clusters = len(cluster_sizes)\n",
    "mean_size = np.mean(cluster_sizes) if cluster_sizes else 0\n",
    "std_size = np.std(cluster_sizes) if cluster_sizes else 0\n",
    "noise_pct = (noise_points / total_points) * 100\n",
    "\n",
    "\n",
    "print(f\"Total points: {total_points}\")\n",
    "print(f\"Noise points: {noise_points}\")\n",
    "print(f\"Noise percentage: {noise_pct:.2f}%\")\n",
    "print(f\"Clusters found (excluding noise): {num_clusters}\")\n",
    "print(f\"Mean cluster size: {mean_size:.2f}\")\n",
    "print(f\"Std cluster size: {std_size:.2f}\")\n",
    "print(f\"Max cluster size: {max(cluster_sizes)}\")\n",
    "print(f\"Min cluster size: {min(cluster_sizes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Prep ---\n",
    "cluster_sizes = [count for cid, count in counts.items() if cid != -1]\n",
    "cluster_ids = [cid for cid in counts.keys() if cid != -1]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Histogram of cluster sizes\n",
    "# ------------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cluster_sizes, bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Number of clusters\")\n",
    "plt.title(\"Histogram of Cluster Sizes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 2. PDF (density estimate) of cluster sizes\n",
    "# ------------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts_hist, bins = np.histogram(cluster_sizes, bins=30, density=True)\n",
    "bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "plt.plot(bin_centers, counts_hist, drawstyle=\"steps-mid\")\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.title(\"Approximate PDF of Cluster Sizes\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Randomized 2D Bubble Chart (biggest clusters first)\n",
    "# ------------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Sort clusters by size (largest first)\n",
    "sorted_sizes = sorted(cluster_sizes, reverse=True)\n",
    "\n",
    "# Random positions for each cluster\n",
    "rng = np.random.default_rng(seed=42)  # reproducible random layout\n",
    "x = rng.uniform(0, 100, len(sorted_sizes))\n",
    "y = rng.uniform(0, 100, len(sorted_sizes))\n",
    "\n",
    "# Bubble radii scaled by cluster size\n",
    "scale_factor = 0.5  # adjust this for bigger/smaller bubbles\n",
    "bubble_sizes = np.array(sorted_sizes) * scale_factor\n",
    "\n",
    "plt.scatter(x, y, s=bubble_sizes, alpha=0.5, c=\"tab:blue\", edgecolors=\"black\")\n",
    "\n",
    "plt.xlabel(\"Random X\")\n",
    "plt.ylabel(\"Random Y\")\n",
    "plt.title(\"Random Bubble Plot of Cluster Sizes (largest clusters drawn bigger)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# T-SNE to 3d for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401, registers 3D projection\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# --- RUN TSNE TO 3D ---\n",
    "print(\"Running t-SNE to 3D...\")\n",
    "tsne = TSNE(\n",
    "    n_components=3,\n",
    "    random_state=42,\n",
    "    perplexity=30,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\"\n",
    ")\n",
    "embeds_3d = tsne.fit_transform(all_embeds)\n",
    "print(\"t-SNE shape:\", embeds_3d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3D PLOT WITH CLUSTER COLORS ---\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Use cluster labels as colors (noise = gray)\n",
    "colors = np.where(cluster_labels == -1, -999, cluster_labels)\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    embeds_3d[:, 0],\n",
    "    embeds_3d[:, 1],\n",
    "    embeds_3d[:, 2],\n",
    "    c=colors,\n",
    "    cmap=\"tab20\",\n",
    "    s=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"t-SNE-1\")\n",
    "ax.set_ylabel(\"t-SNE-2\")\n",
    "ax.set_zlabel(\"t-SNE-3\")\n",
    "ax.set_title(\"t-SNE Projection Colored by HDBSCAN Clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FILTER NON-NOISE POINTS ---\n",
    "mask = cluster_labels != -1\n",
    "embeds_3d_non_noise = embeds_3d[mask]\n",
    "labels_non_noise = cluster_labels[mask]\n",
    "\n",
    "# --- 3D PLOT WITH CLUSTER COLORS (NON-NOISE ONLY) ---\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    embeds_3d_non_noise[:, 0],\n",
    "    embeds_3d_non_noise[:, 1],\n",
    "    embeds_3d_non_noise[:, 2],\n",
    "    c=labels_non_noise,\n",
    "    cmap=\"tab20\",\n",
    "    s=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"t-SNE-1\")\n",
    "ax.set_ylabel(\"t-SNE-2\")\n",
    "ax.set_zlabel(\"t-SNE-3\")\n",
    "ax.set_title(\"t-SNE Projection (Non-Noise HDBSCAN Clusters)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CHOOSE TOP-K CLUSTERS ---\n",
    "k = 40  # change this as needed\n",
    "\n",
    "# Count cluster sizes\n",
    "unique_labels, counts = np.unique(cluster_labels[cluster_labels != -1], return_counts=True)\n",
    "sorted_clusters = [lab for lab, _ in sorted(zip(unique_labels, counts), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "# Keep only top-k\n",
    "top_k_clusters = set(sorted_clusters[:k])\n",
    "\n",
    "mask = np.isin(cluster_labels, list(top_k_clusters))\n",
    "embeds_3d_topk = embeds_3d[mask]\n",
    "labels_topk = cluster_labels[mask]\n",
    "\n",
    "# --- 3D PLOT WITH CLUSTER COLORS (TOP-K ONLY) ---\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    embeds_3d_topk[:, 0],\n",
    "    embeds_3d_topk[:, 1],\n",
    "    embeds_3d_topk[:, 2],\n",
    "    c=labels_topk,\n",
    "    cmap=\"tab20\",\n",
    "    s=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"t-SNE-1\")\n",
    "ax.set_ylabel(\"t-SNE-2\")\n",
    "ax.set_zlabel(\"t-SNE-3\")\n",
    "ax.set_title(f\"t-SNE Projection (Top {k} Largest HDBSCAN Clusters)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# HDBSCAN Centroids -> Distance Clusters (with no noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- META-CLUSTERING WITH AGGLOMERATIVE ---\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# --- USER HYPERPARAMETERS ---\n",
    "# You can toggle these for experiments:\n",
    "USE_CENTROIDS = True         # if False, use medoids (slower, more robust)\n",
    "LINKAGE = \"ward\"             # \"ward\", \"complete\", \"average\", \"single\"\n",
    "N_META = None                # if None, use DIST_THRESHOLD\n",
    "DIST_THRESHOLD = 5.0         # cut dendrogram at this distance if N_META is None\n",
    "\n",
    "# --- COLLECT NON-NOISE CLUSTERS ---\n",
    "hdb_labels = cluster_labels  # from your HDBSCAN run\n",
    "unique_labels = sorted([lbl for lbl in np.unique(hdb_labels) if lbl != -1])\n",
    "\n",
    "# --- COMPUTE CENTROIDS OR MEDOIDS ---\n",
    "summaries = []\n",
    "label_to_index = {}\n",
    "for i, lbl in enumerate(unique_labels):\n",
    "    members = all_embeds[hdb_labels == lbl]\n",
    "    if USE_CENTROIDS:\n",
    "        summary = members.mean(axis=0)\n",
    "    else:\n",
    "        # medoid: point with minimum total distance to others\n",
    "        dists = distance_matrix(members, members)\n",
    "        medoid_idx = np.argmin(dists.sum(axis=1))\n",
    "        summary = members[medoid_idx]\n",
    "    summaries.append(summary)\n",
    "    label_to_index[lbl] = i\n",
    "summaries = np.vstack(summaries)\n",
    "\n",
    "# --- RUN AGGLOMERATIVE ---\n",
    "agg = AgglomerativeClustering(\n",
    "    n_clusters=N_META,\n",
    "    distance_threshold=None if N_META else DIST_THRESHOLD,\n",
    "    linkage=LINKAGE\n",
    ")\n",
    "meta_labels = agg.fit_predict(summaries)\n",
    "\n",
    "# --- MAP HDBSCAN CLUSTERS TO META-CLUSTERS ---\n",
    "cluster_to_meta = {lbl: int(meta_labels[idx]) for lbl, idx in label_to_index.items()}\n",
    "\n",
    "# --- ASSIGN EVERY POINT TO A META-CLUSTER ---\n",
    "final_labels = np.full_like(hdb_labels, fill_value=-1)\n",
    "for lbl in unique_labels:\n",
    "    final_labels[hdb_labels == lbl] = cluster_to_meta[lbl]\n",
    "\n",
    "# --- HANDLE NOISE POINTS (-1) ---\n",
    "noise_mask = (hdb_labels == -1)\n",
    "if noise_mask.any():\n",
    "    noise_points = all_embeds[noise_mask]\n",
    "    dists = distance_matrix(noise_points, summaries)\n",
    "    nearest_idx = np.argmin(dists, axis=1)\n",
    "    final_labels[noise_mask] = meta_labels[nearest_idx]\n",
    "\n",
    "print(\"Final meta-cluster labels shape:\", final_labels.shape)\n",
    "print(\"Unique meta-clusters:\", np.unique(final_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DENDROGRAM VISUALIZATION ---\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Build hierarchical linkage on the summaries (centroids/medoids)\n",
    "Z = linkage(summaries, method=LINKAGE)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendro = dendrogram(\n",
    "    Z,\n",
    "    labels=unique_labels,     # so each leaf corresponds to original HDBSCAN cluster\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0,        # let us control colors later if needed\n",
    ")\n",
    "\n",
    "# Draw horizontal cut line (same as DIST_THRESHOLD from your hyperparams)\n",
    "if N_META is None:\n",
    "    plt.axhline(y=DIST_THRESHOLD, c=\"red\", ls=\"--\", lw=2)\n",
    "\n",
    "plt.title(\"Agglomerative Clustering Dendrogram of HDBSCAN Summaries\")\n",
    "plt.xlabel(\"Original HDBSCAN Clusters\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REPRODUCTION / HOLDOUT TESTS FOR AGGLOMERATIVE -> INFERENCE ---\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Helper: assign new points to meta-clusters using nearest summary\n",
    "def assign_new_points(new_points, summaries, meta_labels):\n",
    "    dists = distance_matrix(new_points, summaries)  # (n_new, n_summaries)\n",
    "    nearest_idx = np.argmin(dists, axis=1)\n",
    "    return meta_labels[nearest_idx], nearest_idx  # returns meta ids and which summary index chosen\n",
    "\n",
    "# ------------------------------\n",
    "# 1) POINT-LEVEL RECONSTRUCTION\n",
    "# ------------------------------\n",
    "def point_reconstruction_test(all_embeds, final_labels, summaries, meta_labels,\n",
    "                              sample_size=1000, random_state=0):\n",
    "    \"\"\"\n",
    "    Randomly sample points and test if nearest-summary assignment reproduces final_labels.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n = all_embeds.shape[0]\n",
    "    sample_size = min(sample_size, n)\n",
    "    idx = rng.choice(n, size=sample_size, replace=False)\n",
    "    sampled_points = all_embeds[idx]\n",
    "    pred_meta, nearest_summary_idx = assign_new_points(sampled_points, summaries, meta_labels)\n",
    "    true_meta = final_labels[idx]\n",
    "\n",
    "    acc = (pred_meta == true_meta).mean()\n",
    "    mismatches = np.where(pred_meta != true_meta)[0]\n",
    "\n",
    "    print(f\"[Point test] Sampled {sample_size} points. Accuracy = {acc:.4f}. Mismatches = {len(mismatches)}\")\n",
    "    if len(mismatches) > 0:\n",
    "        # show up to 10 mismatch examples with helpful context\n",
    "        print(\"Examples (index_in_sample, global_index, true_meta, pred_meta, nearest_summary_idx):\")\n",
    "        for ii in mismatches[:10]:\n",
    "            gi = idx[ii]\n",
    "            print(ii, gi, int(true_meta[ii]), int(pred_meta[ii]), int(nearest_summary_idx[ii]))\n",
    "    return {\"accuracy\": acc, \"n_sample\": sample_size, \"mismatch_indices_sample\": mismatches, \"sample_global_idx\": idx}\n",
    "\n",
    "# ------------------------------\n",
    "# 2) SUMMARY / CLUSTER HOLDOUT\n",
    "# ------------------------------\n",
    "def summary_holdout_test(unique_labels, summaries, meta_labels,\n",
    "                         holdout_k=5, random_state=0):\n",
    "    \"\"\"\n",
    "    Hold out some HDBSCAN clusters (their summaries). Re-fit agglomerative on remaining summaries,\n",
    "    map new meta clusters back to original meta labels (majority vote), then evaluate:\n",
    "      - how well held-out summaries map back to original meta labels\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_summ = summaries.shape[0]\n",
    "    holdout_k = min(holdout_k, n_summ - 1)\n",
    "    # choose holdout by summary index (these correspond to unique_labels order)\n",
    "    holdout_indices = rng.choice(n_summ, size=holdout_k, replace=False)\n",
    "    remain_indices = np.array([i for i in range(n_summ) if i not in set(holdout_indices)], dtype=int)\n",
    "\n",
    "    summaries_remain = summaries[remain_indices]\n",
    "    orig_meta_remain = meta_labels[remain_indices]  # original meta labels for these summaries\n",
    "\n",
    "    # Re-fit agglomerative on remaining summaries using the same hyperparams (LINKAGE / DIST_THRESHOLD / N_META)\n",
    "    agg_retrain = AgglomerativeClustering(\n",
    "        n_clusters=N_META,\n",
    "        distance_threshold=None if N_META else DIST_THRESHOLD,\n",
    "        linkage=LINKAGE\n",
    "    )\n",
    "    new_meta_remain = agg_retrain.fit_predict(summaries_remain)  # labels for each remaining summary\n",
    "\n",
    "    # Build mapping: new_meta_id -> original meta_id (majority vote among members)\n",
    "    new_to_orig = {}\n",
    "    for nm in np.unique(new_meta_remain):\n",
    "        member_idxs = np.where(new_meta_remain == nm)[0]\n",
    "        orig_labels_for_members = orig_meta_remain[member_idxs]\n",
    "        # majority vote\n",
    "        most_common_orig = Counter(orig_labels_for_members).most_common(1)[0][0]\n",
    "        new_to_orig[int(nm)] = int(most_common_orig)\n",
    "\n",
    "    # Assign each held-out summary to nearest remaining summary -> new_meta -> mapped back to original meta\n",
    "    held_summaries = summaries[holdout_indices]\n",
    "    d = distance_matrix(held_summaries, summaries_remain)  # (n_holdout, n_remain)\n",
    "    nearest_remain_idx = np.argmin(d, axis=1)             # indices into summaries_remain\n",
    "    assigned_new_meta = new_meta_remain[nearest_remain_idx]\n",
    "    assigned_orig_meta_via_map = np.array([new_to_orig[int(x)] for x in assigned_new_meta])\n",
    "\n",
    "    # Ground truth for held-out summaries:\n",
    "    true_orig_meta_holdout = meta_labels[holdout_indices]\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc_holdout_summ = (assigned_orig_meta_via_map == true_orig_meta_holdout).mean()\n",
    "\n",
    "    print(f\"[Summary holdout] Held out {len(holdout_indices)} summaries. Accuracy (summary-level) = {acc_holdout_summ:.4f}\")\n",
    "\n",
    "    # Return detailed info so user can inspect mismatches\n",
    "    results = {\n",
    "        \"holdout_summary_indices\": holdout_indices,\n",
    "        \"nearest_remain_idx\": nearest_remain_idx,\n",
    "        \"assigned_new_meta\": assigned_new_meta,\n",
    "        \"assigned_orig_meta_via_map\": assigned_orig_meta_via_map,\n",
    "        \"true_orig_meta_holdout\": true_orig_meta_holdout,\n",
    "        \"accuracy_summary_level\": acc_holdout_summ,\n",
    "        \"new_to_orig_mapping\": new_to_orig,\n",
    "        \"remain_indices\": remain_indices,\n",
    "        \"new_meta_remain\": new_meta_remain\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# ------------------------------\n",
    "# USAGE EXAMPLES (run these lines)\n",
    "# ------------------------------\n",
    "# 1) Point-level test (fast)\n",
    "pt_test_res = point_reconstruction_test(\n",
    "    all_embeds=all_embeds,\n",
    "    final_labels=final_labels,\n",
    "    summaries=summaries,\n",
    "    meta_labels=meta_labels,\n",
    "    sample_size=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Summary-holdout test (re-fits agglomerative on remaining summaries)\n",
    "holdout_res = summary_holdout_test(\n",
    "    unique_labels=unique_labels,\n",
    "    summaries=summaries,\n",
    "    meta_labels=meta_labels,\n",
    "    holdout_k=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Optional deeper check: for each held-out cluster, evaluate point-level assignment accuracy\n",
    "def heldout_points_eval(all_embeds, hdb_labels, holdout_summary_indices, unique_labels,\n",
    "                        summaries_remain, new_meta_remain, new_to_orig_map):\n",
    "    \"\"\"\n",
    "    For each held-out HDBSCAN cluster index (index into summaries / unique_labels),\n",
    "    evaluate the point-level assignment accuracy when assigning points to remaining summaries.\n",
    "    \"\"\"\n",
    "    per_cluster_stats = {}\n",
    "    for sidx in holdout_summary_indices:\n",
    "        original_hdb_label = unique_labels[sidx]\n",
    "        members_mask = (hdb_labels == original_hdb_label)\n",
    "        pts = all_embeds[members_mask]\n",
    "        if pts.shape[0] == 0:\n",
    "            per_cluster_stats[original_hdb_label] = {\"n_points\": 0}\n",
    "            continue\n",
    "        d = distance_matrix(pts, summaries_remain)\n",
    "        nearest_idx = np.argmin(d, axis=1)\n",
    "        assigned_new_meta = new_meta_remain[nearest_idx]\n",
    "        assigned_orig_meta = np.array([new_to_orig_map[int(x)] for x in assigned_new_meta])\n",
    "        true_meta = final_labels[members_mask]\n",
    "        acc = (assigned_orig_meta == true_meta).mean()\n",
    "        per_cluster_stats[original_hdb_label] = {\n",
    "            \"n_points\": pts.shape[0],\n",
    "            \"accuracy\": float(acc),\n",
    "            \"examples\": {\n",
    "                \"true_meta_sample\": true_meta[:5].tolist(),\n",
    "                \"assigned_meta_sample\": assigned_orig_meta[:5].tolist()\n",
    "            }\n",
    "        }\n",
    "    return per_cluster_stats\n",
    "\n",
    "# If you want to run the point-level heldout evaluation, prepare args from holdout_res:\n",
    "# build summaries_remain and new_meta_remain using holdout_res info:\n",
    "remain_indices = holdout_res[\"remain_indices\"]\n",
    "summaries_remain = summaries[remain_indices]\n",
    "new_meta_remain = holdout_res[\"new_meta_remain\"]\n",
    "new_to_orig_map = holdout_res[\"new_to_orig_mapping\"]\n",
    "per_cluster_stats = heldout_points_eval(all_embeds, hdb_labels,\n",
    "                                        holdout_res[\"holdout_summary_indices\"],\n",
    "                                        unique_labels,\n",
    "                                        summaries_remain, new_meta_remain, new_to_orig_map)\n",
    "\n",
    "# Print brief summary\n",
    "print(\"\\nPer-heldout-cluster point-level stats (up to 10 clusters):\")\n",
    "for i, (cl, st) in enumerate(per_cluster_stats.items()):\n",
    "    if i >= 10: break\n",
    "    print(cl, st)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cnn-embeddings)",
   "language": "python",
   "name": "cnn-embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
