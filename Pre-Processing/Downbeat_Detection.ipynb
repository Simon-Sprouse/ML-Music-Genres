{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import madmom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Detect Beats + DownBeats w/ Madmom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "## filename = \"../Songs/bob_marley--redemption_song.mp3\"\n",
    "filename = \"../Songs/dev/tool--ticks_and_leaches.mp3\"\n",
    "y, sr = librosa.load(filename, sr=44100, mono=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Beat + Downbeat tracking\n",
    "# -------------------------------\n",
    "\n",
    "# Process with madmom's RNN downbeat processor\n",
    "proc = madmom.features.downbeats.RNNDownBeatProcessor()(filename)\n",
    "\n",
    "# Decode with a DBN to get sequences of [beat, downbeat]\n",
    "beats = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[3, 4, 5, 7, 11],\n",
    "                                                               fps=100)(proc)\n",
    "\n",
    "# beats is an array of shape (N, 2):\n",
    "#   [:,0] = time (s)\n",
    "#   [:,1] = 1 if downbeat, 0 if beat\n",
    "\n",
    "# Extract beat times and downbeat times\n",
    "beat_times = beats[:,0]\n",
    "downbeat_times = beats[beats[:,1] == 1, 0]\n",
    "\n",
    "# Estimate tempo from inter-beat intervals\n",
    "if len(beat_times) > 1:\n",
    "    ib_intervals = np.diff(beat_times)\n",
    "    tempo = 60.0 / np.median(ib_intervals)\n",
    "else:\n",
    "    tempo = float('nan')\n",
    "\n",
    "print(f\"Estimated tempo: {tempo:.2f} BPM\")\n",
    "print(\"First 20 beats (s):\", beat_times[:20])\n",
    "print(\"First 10 downbeats (s):\", downbeat_times[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Visualize Beat Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER SETTINGS FOR VISUALIZATION WINDOW ---\n",
    "start_time = 90.0  # Start time in seconds\n",
    "window_length = 20.0  # Window length in seconds\n",
    "end_time = start_time + window_length\n",
    "\n",
    "print(f\"\\nVisualization window: {start_time}s to {end_time}s\")\n",
    "\n",
    "# Filter beats and downbeats within the specified window\n",
    "beats_in_window = beat_times[(beat_times >= start_time) & (beat_times <= end_time)]\n",
    "downbeats_in_window = downbeat_times[(downbeat_times >= start_time) & (downbeat_times <= end_time)]\n",
    "print(f\"Beats in window: {len(beats_in_window)}, Downbeats in window: {len(downbeats_in_window)}\")\n",
    "\n",
    "\n",
    "### PLOT 1 - Beat Times overlayed on raw Audio Form ###\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Extract audio segment for the specified window\n",
    "start_sample = int(start_time * sr)\n",
    "end_sample = int(end_time * sr)\n",
    "y_window = y[start_sample:end_sample]\n",
    "\n",
    "# Plot windowed waveform\n",
    "time_axis_window = np.linspace(start_time, end_time, len(y_window))\n",
    "plt.plot(time_axis_window, y_window, alpha=1, color='blue', linewidth=0.8)\n",
    "\n",
    "# Overlay regular beat markers (excluding downbeats)\n",
    "regular_beats = np.setdiff1d(beats_in_window, downbeats_in_window)\n",
    "plt.vlines(regular_beats, np.min(y_window), np.max(y_window), \n",
    "           color='red', alpha=0.4, linestyle='-', \n",
    "           linewidth=2, label='Beats')\n",
    "\n",
    "# Overlay downbeat markers\n",
    "plt.vlines(downbeats_in_window, np.min(y_window), np.max(y_window), \n",
    "           color='green', alpha=0.7, linestyle='-', \n",
    "           linewidth=2, label='Downbeats')\n",
    "\n",
    "plt.xlim(start_time, end_time)\n",
    "plt.ylim([np.min(y_window) * 1.1, np.max(y_window) * 1.1])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(f'Audio Waveform with Beat & Downbeat Detection ({start_time}-{end_time}s)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### PLOT 2 - Time intervals between Beat Frames ###\n",
    "beat_intervals = np.diff(beat_times)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(beat_times[1:], beat_intervals, 'o-', alpha=0.7, markersize=3)\n",
    "\n",
    "# Highlight the window region\n",
    "window_mask = (beat_times[1:] >= start_time) & (beat_times[1:] <= end_time)\n",
    "if np.any(window_mask):\n",
    "    plt.plot(beat_times[1:][window_mask], beat_intervals[window_mask], \n",
    "             'ro-', alpha=0.9, markersize=4, label=f'Window ({start_time}-{end_time}s)')\n",
    "\n",
    "plt.axhline(y=60/tempo, color='red', linestyle='--', alpha=0.8, \n",
    "            label=f'Expected interval: {60/tempo:.3f}s')\n",
    "plt.axvspan(start_time, end_time, alpha=0.2, color='yellow', label='Analysis window')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Beat Interval (s)')\n",
    "plt.title('Beat Interval Analysis (Full Song)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Add Metronome (detected) to Soundtrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_clicks_dual(y, sr, beat_times, downbeat_times,\n",
    "                        click_high_path=\"./perc_high.wav\",\n",
    "                        click_low_path=\"./perc_low.wav\",\n",
    "                        out_path=None):\n",
    "    \"\"\"\n",
    "    Overlay high clicks on downbeats and low clicks on other beats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Original audio signal (mono).\n",
    "    sr : int\n",
    "        Sample rate of the audio.\n",
    "    beat_times : np.ndarray\n",
    "        Array of all beat times in seconds.\n",
    "    downbeat_times : np.ndarray\n",
    "        Array of downbeat times in seconds.\n",
    "    click_high_path : str\n",
    "        Path to a high click .wav file (downbeats).\n",
    "    click_low_path : str\n",
    "        Path to a low click .wav file (other beats).\n",
    "    out_path : str or None\n",
    "        If provided, saves the output audio to this path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_out : np.ndarray\n",
    "        Audio with clicks overlayed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load clicks and match sample rate\n",
    "    click_high, _ = librosa.load(click_high_path, sr=sr, mono=True)\n",
    "    click_low, _ = librosa.load(click_low_path, sr=sr, mono=True)\n",
    "\n",
    "    click_gain = 2\n",
    "    click_high *= click_gain\n",
    "    click_low *= click_gain\n",
    "\n",
    "    y_out = np.copy(y)\n",
    "\n",
    "    # Overlay clicks\n",
    "    downbeat_set = set(np.round(downbeat_times * 1000).astype(int))  # millisecond rounding for matching\n",
    "    for t in beat_times:\n",
    "        start_sample = int(t * sr)\n",
    "        if start_sample >= len(y_out):\n",
    "            continue\n",
    "\n",
    "        # Determine if this beat is a downbeat\n",
    "        key = int(round(t * 1000))\n",
    "        click = click_high if key in downbeat_set else click_low\n",
    "\n",
    "        end_sample = start_sample + len(click)\n",
    "        if end_sample > len(y_out):\n",
    "            click_segment = click[: len(y_out) - start_sample]\n",
    "        else:\n",
    "            click_segment = click\n",
    "\n",
    "        # Mix click into signal\n",
    "        y_out[start_sample:start_sample + len(click_segment)] += click_segment\n",
    "\n",
    "    # Normalize\n",
    "    max_val = np.max(np.abs(y_out))\n",
    "    if max_val > 1.0:\n",
    "        y_out = y_out / max_val\n",
    "\n",
    "    # Save if requested\n",
    "    if out_path is not None:\n",
    "        sf.write(out_path, y_out, sr)\n",
    "\n",
    "    return y_out\n",
    "\n",
    "\n",
    "# Overlay dual clicks\n",
    "y_debug = overlay_clicks_dual(y, sr, beat_times, downbeat_times,\n",
    "                              click_high_path=\"./perc_high.wav\",\n",
    "                              click_low_path=\"./perc_low.wav\",\n",
    "                              out_path=\"../Results/debug_with_dual_clicks.wav\")\n",
    "\n",
    "print(\"Debug audio with high/low clicks saved to debug_with_dual_clicks.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_audio)",
   "language": "python",
   "name": "ml_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
