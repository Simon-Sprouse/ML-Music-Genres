{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrubberband as pyrb\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import madmom\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "print(pyrb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Load audio and Detect Key + Downbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../Songs/bob_marley--redemption_song.mp3\"\n",
    "y, sr = librosa.load(filename, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with madmom's RNN downbeat processor\n",
    "proc = madmom.features.downbeats.RNNDownBeatProcessor()(filename)\n",
    "\n",
    "# Decode with a DBN to get sequences of [beat, downbeat]\n",
    "beats = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[3, 4],\n",
    "                                                               fps=100)(proc)\n",
    "\n",
    "# beats is an array of shape (N, 2):\n",
    "#   [:,0] = time (s)\n",
    "#   [:,1] = 1 if downbeat, 0 if beat\n",
    "\n",
    "# Extract beat times and downbeat times\n",
    "beat_times = beats[:,0]\n",
    "downbeat_times = beats[beats[:,1] == 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Make Beat-aligned Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def make_downbeat_aligned_images(y, sr, downbeat_times, time_bins_per_downbeat=32, downbeats_in_image=4):\n",
    "    \"\"\"\n",
    "    Create log-mel spectrogram images aligned to downbeats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Audio signal.\n",
    "    sr : int\n",
    "        Sampling rate.\n",
    "    downbeat_times : list of float\n",
    "        Times (in seconds) of detected downbeats.\n",
    "    time_bins_per_downbeat : int\n",
    "        Number of spectrogram frames allocated between two downbeats.\n",
    "    downbeats_in_image : int\n",
    "        How many downbeats per image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : list of np.ndarray\n",
    "        Each image has shape (128, time_bins_per_downbeat * downbeats_in_image).\n",
    "    \"\"\"\n",
    "    # Compute mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Map downbeat times -> spectrogram frames\n",
    "    hop_length = 512\n",
    "    frame_times = librosa.frames_to_time(np.arange(S_db.shape[1]), sr=sr, hop_length=hop_length)\n",
    "    downbeat_frames = np.searchsorted(frame_times, downbeat_times)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(downbeat_frames) - 1):\n",
    "        start, end = downbeat_frames[i], downbeat_frames[i + 1]\n",
    "        segment = S_db[:, start:end]\n",
    "\n",
    "        if segment.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Resample along time axis to exactly time_bins_per_downbeat\n",
    "        x_old = np.linspace(0, 1, segment.shape[1])\n",
    "        x_new = np.linspace(0, 1, time_bins_per_downbeat)\n",
    "        segment_resampled = np.vstack([\n",
    "            np.interp(x_new, x_old, row) for row in segment\n",
    "        ])\n",
    "\n",
    "        segments.append(segment_resampled)\n",
    "\n",
    "    # Group consecutive segments into images\n",
    "    images = []\n",
    "    for i in range(0, len(segments) - downbeats_in_image + 1, downbeats_in_image):\n",
    "        image = np.hstack(segments[i:i + downbeats_in_image])\n",
    "        images.append(image)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# y, sr = librosa.load(\"song.wav\", sr=None)\n",
    "# downbeat_times = [0.0, 1.0, 2.0, 3.0, 4.0]  # dummy\n",
    "imgs = make_downbeat_aligned_images(y, sr, downbeat_times, time_bins_per_downbeat=128, downbeats_in_image=1)\n",
    "print(len(imgs), imgs[0].shape)  # -> (N, (128, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Test Spectrogram Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = imgs[27]\n",
    "\n",
    "# If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "if np.issubdtype(arr.dtype, np.floating):\n",
    "    arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "\n",
    "pil_img = Image.fromarray(arr)\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Download Image Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(imgs):\n",
    "    # If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "    if np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "    \n",
    "    pil_img = Image.fromarray(arr)\n",
    "    pil_img.save(f\"../Results/spectrogram_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Make Beat-aligned Chromagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def make_downbeat_aligned_chromagrams(y, sr, downbeat_times, time_bins_per_downbeat=128, downbeats_in_image=1):\n",
    "    \"\"\"\n",
    "    Create chromagram images aligned to downbeats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Audio signal.\n",
    "    sr : int\n",
    "        Sampling rate.\n",
    "    downbeat_times : list of float\n",
    "        Times (in seconds) of detected downbeats.\n",
    "    time_bins_per_downbeat : int\n",
    "        Number of chroma frames allocated between two downbeats.\n",
    "    downbeats_in_image : int\n",
    "        How many downbeats per image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : list of np.ndarray\n",
    "        Each image has shape (12, time_bins_per_downbeat * downbeats_in_image).\n",
    "    \"\"\"\n",
    "    # Compute chromagram\n",
    "    hop_length = 512\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Map downbeat times -> chroma frames\n",
    "    frame_times = librosa.frames_to_time(np.arange(chroma.shape[1]), sr=sr, hop_length=hop_length)\n",
    "    downbeat_frames = np.searchsorted(frame_times, downbeat_times)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(downbeat_frames) - 1):\n",
    "        start, end = downbeat_frames[i], downbeat_frames[i + 1]\n",
    "        segment = chroma[:, start:end]\n",
    "\n",
    "        if segment.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Resample along time axis to exactly time_bins_per_downbeat\n",
    "        x_old = np.linspace(0, 1, segment.shape[1])\n",
    "        x_new = np.linspace(0, 1, time_bins_per_downbeat)\n",
    "        segment_resampled = np.vstack([\n",
    "            np.interp(x_new, x_old, row) for row in segment\n",
    "        ])\n",
    "\n",
    "        segments.append(segment_resampled)\n",
    "\n",
    "    # Group consecutive segments into images\n",
    "    images = []\n",
    "    for i in range(0, len(segments) - downbeats_in_image + 1, downbeats_in_image):\n",
    "        image = np.hstack(segments[i:i + downbeats_in_image])\n",
    "        images.append(image)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# y, sr = librosa.load(\"song.wav\", sr=None)\n",
    "# downbeat_times = [0.0, 1.0, 2.0, 3.0, 4.0]  # dummy\n",
    "imgs = make_downbeat_aligned_chromagrams(y, sr, downbeat_times, time_bins_per_downbeat=128, downbeats_in_image=1)\n",
    "print(len(imgs), imgs[0].shape)  # -> (N, (12, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Test Raw Chromagram Output (small on purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = imgs[2]\n",
    "\n",
    "# If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "if np.issubdtype(arr.dtype, np.floating):\n",
    "    arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "\n",
    "pil_img = Image.fromarray(arr)\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Visualize Chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chromagram_image(chroma_img, sr=22050, hop_length=512, cmap=\"magma\", y_pixels=12):\n",
    "    \"\"\"\n",
    "    Visualize a chromagram image with matplotlib.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chroma_img : np.ndarray\n",
    "        A single chromagram image, shape (12, T).\n",
    "    sr : int\n",
    "        Sampling rate (optional, used for time axis labeling).\n",
    "    hop_length : int\n",
    "        Hop length (optional, used for time axis labeling).\n",
    "    cmap : str\n",
    "        Matplotlib colormap.\n",
    "    y_pixels : int\n",
    "        Number of vertical pixels for display (default 12).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(chroma_img, \n",
    "               aspect='auto', \n",
    "               origin='lower', \n",
    "               cmap=cmap,\n",
    "               extent=[0, chroma_img.shape[1], 0, y_pixels])\n",
    "    plt.colorbar(label=\"Chroma Energy\")\n",
    "    plt.xlabel(\"Time (resampled bins)\")\n",
    "    plt.ylabel(\"Pitch Class\")\n",
    "    plt.yticks(np.linspace(0.5, y_pixels - 0.5, 12), \n",
    "               [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \n",
    "                \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"])\n",
    "    plt.title(\"Downbeat-Aligned Chromagram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Suppose you already ran:\n",
    "imgs = make_downbeat_aligned_chromagrams(y, sr, downbeat_times)\n",
    "\n",
    "# Visualize the first chromagram\n",
    "plot_chromagram_image(imgs[0], cmap=\"inferno\", y_pixels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (process_audio)",
   "language": "python",
   "name": "process_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
