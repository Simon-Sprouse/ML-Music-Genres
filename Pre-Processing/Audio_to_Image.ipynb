{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrubberband as pyrb\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import madmom\n",
    "from madmom.features.key import CNNKeyRecognitionProcessor, key_prediction_to_label\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(pyrb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Load audio and Detect Key + Downbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../Songs/bob_marley--redemption_song.mp3\"\n",
    "y, sr = librosa.load(filename, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with madmom's RNN downbeat processor\n",
    "downbeat_proc = madmom.features.downbeats.RNNDownBeatProcessor()(filename)\n",
    "\n",
    "# Decode with a DBN to get sequences of [beat, downbeat]\n",
    "beats = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[3, 4],\n",
    "                                                               fps=100)(downbeat_proc)\n",
    "\n",
    "# beats is an array of shape (N, 2):\n",
    "#   [:,0] = time (s)\n",
    "#   [:,1] = 1 if downbeat, 0 if beat\n",
    "\n",
    "# Extract beat times and downbeat times\n",
    "beat_times = beats[:,0]\n",
    "downbeat_times = beats[beats[:,1] == 1, 0]\n",
    "\n",
    "\n",
    "key_proc = CNNKeyRecognitionProcessor()\n",
    "\n",
    "# run on an audio file\n",
    "prediction = key_proc(filename)\n",
    "\n",
    "# convert to readable label\n",
    "key = key_prediction_to_label(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Make Beat-aligned Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def make_downbeat_aligned_images(y, sr, downbeat_times, time_bins_per_downbeat=32, downbeats_in_image=4):\n",
    "    \"\"\"\n",
    "    Create log-mel spectrogram images aligned to downbeats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Audio signal.\n",
    "    sr : int\n",
    "        Sampling rate.\n",
    "    downbeat_times : list of float\n",
    "        Times (in seconds) of detected downbeats.\n",
    "    time_bins_per_downbeat : int\n",
    "        Number of spectrogram frames allocated between two downbeats.\n",
    "    downbeats_in_image : int\n",
    "        How many downbeats per image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : list of np.ndarray\n",
    "        Each image has shape (128, time_bins_per_downbeat * downbeats_in_image).\n",
    "    \"\"\"\n",
    "    # Compute mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Map downbeat times -> spectrogram frames\n",
    "    hop_length = 512\n",
    "    frame_times = librosa.frames_to_time(np.arange(S_db.shape[1]), sr=sr, hop_length=hop_length)\n",
    "    downbeat_frames = np.searchsorted(frame_times, downbeat_times)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(downbeat_frames) - 1):\n",
    "        start, end = downbeat_frames[i], downbeat_frames[i + 1]\n",
    "        segment = S_db[:, start:end]\n",
    "\n",
    "        if segment.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Resample along time axis to exactly time_bins_per_downbeat\n",
    "        x_old = np.linspace(0, 1, segment.shape[1])\n",
    "        x_new = np.linspace(0, 1, time_bins_per_downbeat)\n",
    "        segment_resampled = np.vstack([\n",
    "            np.interp(x_new, x_old, row) for row in segment\n",
    "        ])\n",
    "\n",
    "        segments.append(segment_resampled)\n",
    "\n",
    "    # Group consecutive segments into images\n",
    "    images = []\n",
    "    for i in range(0, len(segments) - downbeats_in_image + 1, downbeats_in_image):\n",
    "        image = np.hstack(segments[i:i + downbeats_in_image])\n",
    "        images.append(image)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# y, sr = librosa.load(\"song.wav\", sr=None)\n",
    "# downbeat_times = [0.0, 1.0, 2.0, 3.0, 4.0]  # dummy\n",
    "imgs = make_downbeat_aligned_images(y, sr, downbeat_times, time_bins_per_downbeat=128, downbeats_in_image=1)\n",
    "print(len(imgs), imgs[0].shape)  # -> (N, (128, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Test Spectrogram Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = imgs[2]\n",
    "\n",
    "# If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "if np.issubdtype(arr.dtype, np.floating):\n",
    "    arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "\n",
    "pil_img = Image.fromarray(arr)\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Download Image Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(imgs):\n",
    "    # If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "    if np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "    \n",
    "    pil_img = Image.fromarray(arr)\n",
    "    pil_img.save(f\"../Results/spectrogram_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Make Beat-aligned Chromagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def make_downbeat_aligned_chromagrams(y, sr, downbeat_times, time_bins_per_downbeat=128, downbeats_in_image=1):\n",
    "    \"\"\"\n",
    "    Create chromagram images aligned to downbeats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Audio signal.\n",
    "    sr : int\n",
    "        Sampling rate.\n",
    "    downbeat_times : list of float\n",
    "        Times (in seconds) of detected downbeats.\n",
    "    time_bins_per_downbeat : int\n",
    "        Number of chroma frames allocated between two downbeats.\n",
    "    downbeats_in_image : int\n",
    "        How many downbeats per image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : list of np.ndarray\n",
    "        Each image has shape (12, time_bins_per_downbeat * downbeats_in_image).\n",
    "    \"\"\"\n",
    "    # Compute chromagram\n",
    "    hop_length = 512\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Map downbeat times -> chroma frames\n",
    "    frame_times = librosa.frames_to_time(np.arange(chroma.shape[1]), sr=sr, hop_length=hop_length)\n",
    "    downbeat_frames = np.searchsorted(frame_times, downbeat_times)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(downbeat_frames) - 1):\n",
    "        start, end = downbeat_frames[i], downbeat_frames[i + 1]\n",
    "        segment = chroma[:, start:end]\n",
    "\n",
    "        if segment.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Resample along time axis to exactly time_bins_per_downbeat\n",
    "        x_old = np.linspace(0, 1, segment.shape[1])\n",
    "        x_new = np.linspace(0, 1, time_bins_per_downbeat)\n",
    "        segment_resampled = np.vstack([\n",
    "            np.interp(x_new, x_old, row) for row in segment\n",
    "        ])\n",
    "\n",
    "        segments.append(segment_resampled)\n",
    "\n",
    "    # Group consecutive segments into images\n",
    "    images = []\n",
    "    for i in range(0, len(segments) - downbeats_in_image + 1, downbeats_in_image):\n",
    "        image = np.hstack(segments[i:i + downbeats_in_image])\n",
    "        images.append(image)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# y, sr = librosa.load(\"song.wav\", sr=None)\n",
    "# downbeat_times = [0.0, 1.0, 2.0, 3.0, 4.0]  # dummy\n",
    "imgs = make_downbeat_aligned_chromagrams(y, sr, downbeat_times, time_bins_per_downbeat=128, downbeats_in_image=1)\n",
    "print(len(imgs), imgs[0].shape)  # -> (N, (12, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Test Raw Chromagram Output (small on purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = imgs[2]\n",
    "\n",
    "# If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "if np.issubdtype(arr.dtype, np.floating):\n",
    "    arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "\n",
    "pil_img = Image.fromarray(arr)\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Visualize Chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chromagram_image(chroma_img, sr=22050, hop_length=512, cmap=\"magma\", y_pixels=12):\n",
    "    \"\"\"\n",
    "    Visualize a chromagram image with matplotlib.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chroma_img : np.ndarray\n",
    "        A single chromagram image, shape (12, T).\n",
    "    sr : int\n",
    "        Sampling rate (optional, used for time axis labeling).\n",
    "    hop_length : int\n",
    "        Hop length (optional, used for time axis labeling).\n",
    "    cmap : str\n",
    "        Matplotlib colormap.\n",
    "    y_pixels : int\n",
    "        Number of vertical pixels for display (default 12).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(chroma_img, \n",
    "               aspect='auto', \n",
    "               origin='lower', \n",
    "               cmap=cmap,\n",
    "               extent=[0, chroma_img.shape[1], 0, y_pixels])\n",
    "    plt.colorbar(label=\"Chroma Energy\")\n",
    "    plt.xlabel(\"Time (interpolated bins)\")\n",
    "    plt.ylabel(\"Pitch Class\")\n",
    "    plt.yticks(np.linspace(0.5, y_pixels - 0.5, 12), \n",
    "               [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \n",
    "                \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"])\n",
    "    plt.title(\"Chromagram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Suppose you already ran:\n",
    "imgs = make_downbeat_aligned_chromagrams(y, sr, downbeat_times)\n",
    "\n",
    "# Visualize the first chromagram\n",
    "plot_chromagram_image(imgs[0], cmap=\"inferno\", y_pixels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# Make Beat-Unaligned Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def make_fixed_size_spectrogram(y, sr, n_seconds=3.0, width=128, n_mels=128):\n",
    "    \"\"\"\n",
    "    Create a fixed-size mel spectrogram image with exactly (n_mels, width) shape.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Audio signal.\n",
    "    sr : int\n",
    "        Sample rate.\n",
    "    n_seconds : float\n",
    "        Duration (in seconds) of each spectrogram window.\n",
    "    width : int\n",
    "        Number of time bins (pixels) in the spectrogram.\n",
    "    n_mels : int\n",
    "        Number of mel frequency bins.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    specs : list of np.ndarray\n",
    "        Each spectrogram of shape (n_mels, width).\n",
    "    \"\"\"\n",
    "    # Compute hop_length so that n_seconds -> width frames\n",
    "    hop_length = int(round((n_seconds * sr) / width))\n",
    "\n",
    "    # Frame length: usually 2–4× hop length for STFT stability\n",
    "    n_fft = hop_length * 4\n",
    "\n",
    "    # Compute mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr,\n",
    "                                         n_fft=n_fft,\n",
    "                                         hop_length=hop_length,\n",
    "                                         n_mels=n_mels)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    # Segment into non-overlapping windows of size `width`\n",
    "    specs = []\n",
    "    for i in range(0, mel_db.shape[1] - width + 1, width):\n",
    "        window = mel_db[:, i:i + width]\n",
    "        specs.append(window)\n",
    "\n",
    "    return specs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# y, sr = librosa.load(\"song.wav\", sr=None)\n",
    "spectros = make_fixed_size_spectrogram(y, sr, n_seconds=3.0, width=128, n_mels=128)\n",
    "print(spectros[0].shape)  # (64, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = spectros[30]\n",
    "\n",
    "# If the array is float (e.g. values in [0,1] or arbitrary floats), scale to 0-255\n",
    "if np.issubdtype(arr.dtype, np.floating):\n",
    "    arr = (255 * (arr - arr.min()) / (arr.max() - arr.min())).astype(np.uint8)\n",
    "\n",
    "pil_img = Image.fromarray(arr)\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "# Make Beat-Unaligned Chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def make_fixed_size_chromagrams(y, sr, n_seconds=3.0, width=128, hop_length=512):\n",
    "    \"\"\"\n",
    "    Create fixed-size chromagram images (not beat aligned) using\n",
    "    a single chromagram computation + interpolation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Audio signal.\n",
    "    sr : int\n",
    "        Sampling rate.\n",
    "    n_seconds : float\n",
    "        Duration (in seconds) of each window.\n",
    "    width : int\n",
    "        Number of chroma frames allocated per window (output width).\n",
    "    hop_length : int\n",
    "        Hop length used to compute the base chromagram.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : list of np.ndarray\n",
    "        Each image has shape (12, width).\n",
    "    \"\"\"\n",
    "    # Compute base chromagram once\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Get frame times\n",
    "    frame_times = librosa.frames_to_time(\n",
    "        np.arange(chroma.shape[1]), sr=sr, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    # Window length in frames\n",
    "    window_length_frames = int(round(n_seconds * sr / hop_length))\n",
    "\n",
    "    images = []\n",
    "    for start in range(0, chroma.shape[1] - window_length_frames + 1, window_length_frames):\n",
    "        end = start + window_length_frames\n",
    "        segment = chroma[:, start:end]\n",
    "\n",
    "        if segment.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Resample to fixed width\n",
    "        x_old = np.linspace(0, 1, segment.shape[1])\n",
    "        x_new = np.linspace(0, 1, width)\n",
    "        segment_resampled = np.vstack([\n",
    "            np.interp(x_new, x_old, row) for row in segment\n",
    "        ])\n",
    "\n",
    "        images.append(segment_resampled)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# y, sr = librosa.load(\"song.wav\", sr=None)\n",
    "chromas = make_fixed_size_chromagrams(y, sr, n_seconds=3.0, width=128, hop_length=512)\n",
    "print(len(chromas), chromas[0].shape)  # -> (N, (12, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chromagram_image(chromas[3], cmap=\"inferno\", y_pixels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "# Make Key-Aligned Chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Map note names to chroma indices\n",
    "NOTE_TO_INDEX = {\n",
    "    'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3,\n",
    "    'E': 4, 'F': 5, 'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8, 'Ab': 8,\n",
    "    'A': 9, 'A#': 10, 'Bb': 10, 'B': 11\n",
    "}\n",
    "\n",
    "def rotate_chroma_to_C_major(chroma_image, key_str):\n",
    "    \"\"\"\n",
    "    Rotate a chromagram so that the song is aligned to C major (or A minor).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chroma_image : np.ndarray\n",
    "        A chromagram image with shape (12, width).\n",
    "    key_str : str\n",
    "        Detected key, e.g. 'G major' or 'A minor'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rotated : np.ndarray\n",
    "        Chromagram rotated so tonic aligns with C major.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse key\n",
    "    parts = key_str.strip().split()\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Unexpected key format: {key_str}\")\n",
    "    tonic, mode = parts[0], parts[1].lower()\n",
    "\n",
    "    if tonic not in NOTE_TO_INDEX:\n",
    "        raise ValueError(f\"Unrecognized tonic: {tonic}\")\n",
    "\n",
    "    tonic_index = NOTE_TO_INDEX[tonic]\n",
    "\n",
    "    if mode == \"major\":\n",
    "        offset = tonic_index\n",
    "    elif mode == \"minor\":\n",
    "        # shift to relative major (3 semitones up)\n",
    "        offset = (tonic_index + 3) % 12\n",
    "    else:\n",
    "        raise ValueError(f\"Mode must be 'major' or 'minor', got: {mode}\")\n",
    "\n",
    "    # Rotate so tonic maps to C (index 0)\n",
    "    rotated = np.roll(chroma_image, -offset, axis=0)\n",
    "\n",
    "    return rotated\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Suppose we have a chromagram (12, 128)\n",
    "chroma_example = np.random.rand(12, 128)\n",
    "rotated = rotate_chroma_to_C_major(chromas[3], \"G major\")\n",
    "print(rotated.shape)  # (12, 128)\n",
    "\n",
    "plot_chromagram_image(rotated, cmap=\"inferno\", y_pixels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (process_audio)",
   "language": "python",
   "name": "process_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
